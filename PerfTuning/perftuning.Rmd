---
title: "Examining Performance Tuning Benchmarks v1"
---

This document is intended to address a potential concern with the performance tuning
information posted [here](https://github.com/jreynolds01/SQL-Server-R-Services-Samples/tree/master/PerfTuning)

I have not reviewed all the tests, but the issue I did notice is in the third section 
[Using Integer vs. String in Formula](https://msdn.microsoft.com/en-us/library/mt723571.aspx)

The concerning part is this:

```
## Using Integer vs. String in Formula


In this experiment, rxLinMod was used with the `airline` table 
(where `DayOfWeek` is a string) and `airlineWithIndex` (where `DayOfWeek` is an integer). 
For the first case, `colInfo` was used to specify the factor levels (Monday, Tuesday, …). 
For the second, `colInfo` was not specified. In both case, the same formula was used. 
The formula used is `ArrDelay ~ CRSDepTime + DayOfWeek`. 
**The following results clearly shows the benefit of using integer vs string:**



| Table name         | Test name | Average time |
|:-------------------|----------:|-------------:|
| airline            | FactorCol | 10.72        |
| airlineWithIntCol  |    IntCol | 3.4475       |
```

**(Emphasis added on my part.)**

The issue is that this investigation confounds two different issues that could drive
the difference in performance:

- The storage type in the database
- The complexity of the resulting model that is estimated. Specifically, the `FactorCol` model
is more complex because the `DayOfWeek` variable is translated to a set of 6 'dummy' variables
in the model. The `IntCol` model only uses 1 variable to cover the `DayOfWeek` variable. 

The language above suggests that the effect of the speed up is because of the storage type
in the database, but it could be driven by the differences in models.

This document goes through independently testing those two different things (among a few additional pieces below).

The outline of the document is to start by replicating the datasources from the `runtests.R` script, 
proceed to show that the models are in fact different, and then test whether the difference
observed above is due to the model or the data source. Once that is completed, then a few additional
tests are done, but those tests don't change the main upshot of the results.

First, we source the functions used to do the benchmarks, and test them.

```{r source-runtests}
source('runtests.R')
rxOptions(reportProgress = 0)
```

Let's replicate the prior examples:

```{r run-sourced-code}
myTestsToRun <- c('FactorCol','IntCol')
runTests(myTestsToRun, 'Run1', 1, 500000L) 
```

In my local tests on a D4 Windoes Data Science Virtual machine (with no optimizations), 
this was not quite as fast, but took approximately 20s on average for `FactorCol`, and 7s for `IntCol`.
So, I was able to approximately replicate this effect, with the `FactorCol` test taking approximately
`r round(10.72/3.4475, 1)` times longer than `IntCol`.

## New approach

The code to do the benchmarks is a little complicated in `runtests.R`, and does not allow for inspecting the actual models for equivalency,
so I will use a slightly different approach. I will use the `microbenchmark` package to do the benchmarks.

Let's make sure it's installed:

```{r load-mb}
if(!require(microbenchmark)) {
	install.packages('microbenchmark')
	library(microbenchmark)
}

```

From `runtests.R`, the key code is in the `runtests` function, and I've replicated the version as of 12/16/2016 below:

```{r runtests-code, eval = FALSE}
if ("FactorCol" %in% testsToRun) {
    colInfo <- getColInfoForSqlDataSource(airlineXdf, c("DayOfWeek"), FALSE)
    formula <- ArrDelay ~ CRSDepTime + DayOfWeek
    airlineTable <- RxSqlServerData(table="airline", connectionString = sqlConnString, colInfo = colInfo)
    cat("\nRunning FactorCol Test. Using airline table.", "\n")
    testResult <- runTest(resultDir, "rxLinMod", formula = formula, dataSource = airlineTable, transformFunc = NULL, transformVars = NULL, runs=5)
    cat("Average Time: ", testResult$avgTime, "\n")
    times <- processTimings(testResult)
    if (printTimeSplits) {
      print(times)
    }
  }
  
  if ("IntCol" %in% testsToRun) {
    formula <- ArrDelay ~ CRSDepTime + DayOfWeek
    airlineWithIntCol <- RxSqlServerData(table="airlineWithIntCol", connectionString = sqlConnString)
    cat("\nRunning IntCol Test. Using airlineWithIntCol table.", "\n")
    testResult <- runTest(resultDir, "rxLinMod", formula = formula, dataSource = airlineWithIntCol, transformFunc = NULL, transformVars = NULL, runs=5)
    cat("Average Time: ", testResult$avgTime, "\n")
    times <- processTimings(testResult)
    if (printTimeSplits) {
      print(times)
    }
  }
```

Let's run the benchmarks with `microbenchmark` instead to try to speed up the process. 
First, set up the arguments that are passed to `runtests()`, and set up the compute context:

```{r set-context-and-args}
numTasks = 1            ## this is an argument to runTests() (used in the computeContext)
rowsPerRead = 500000L   ## this is an argument to runTests() (This is not used)
bench_times = 5L        ## number of times based on runTests()
rxOptions(reportProgress = 0)
sqlcc <- getSqlComputeContext(numTasks, sqlConnString)
rxSetComputeContext(sqlcc)
```

Next, let's define the two data sources to be exactly the same as before:

```{r define-data-sources}
colInfo <- getColInfoForSqlDataSource(airlineXdf, c("DayOfWeek"), FALSE)
airlineTable <- RxSqlServerData(table="airline", connectionString = sqlConnString, colInfo = colInfo)
airlineWithIntCol <- RxSqlServerData(table="airlineWithIntCol", connectionString = sqlConnString)
```

And let's double check to see that the variables look like we'd expect:

```{r view-data}
varsToKeep = c('ArrDelay','CRSDepTime','DayOfWeek')
rxGetVarInfo(airlineTable, varsToKeep = varsToKeep)
rxGetVarInfo(airlineWithIntCol, varsToKeep = varsToKeep)
```

Next, let's set up the variable as indicated in `runtests()`:

```{r define-formula}
formula0 <- ArrDelay ~ CRSDepTime + DayOfWeek
```

And, now let's reconstuct the models to show that they are different:

```{r estimate-models}
factorModel <- rxLinMod(formula0, data = airlineTable, reportProgress = 0)
intModel <- rxLinMod(formula0, data = airlineWithIntCol, reportProgress = 0)
```

To see that they're different, you just need to check the coefficients:

```{r view-coefs}
coef(factorModel)
coef(intModel)
```

Clearly, the number of coefficients is not equal in the two, so the models are not equivalent. 
`factorModel` contains `r length(coef(factorModel))` coefficients while `intModel` contains 
`r length(coef(intModel))` coefficients.

Since both the models are different and the storage type is different, we should try to determine which is 
driving the effect. We can think of the possible outomes in the context of a 2 (data source) x 2 (model) 
design, where we could have the different options:

- character storage, categorical predictor (`factorModel` above)
- int storage, int predictor (`intModel` above)
- character storage, int predictor (not shown above)
- int storage, factor predictor (not shown above)

You can think of the observed models as being in a diagonal of a 2 x 2 table, where rows correspond to different predictor
types, and different columns correspond to different storage types.

|   |int|char|
|--:|:--:|:--:|
|        int| x  |    |
|Categorical|    |  x |

We can't tell if the difference is due to the predictor type, or the storage type. This becomes a little more complicated 
when we can consider that there are different points in time when we
can do the variable casting, but for now, let's hold that constant and test 
the effects of forcing dummy variable creation during model estimation.

## Check Model Equivalencies

First, let's make sure the models are equivalent.

Let's estimate a model that uses character storage, but a numeric predictor:

```{r test-intmodels}
charStor_intPred_Model <- rxLinMod(ArrDelay ~ CRSDepTime + as.integer(DayOfWeek), data = airlineTable)
coef(intModel)
coef(charStor_intPred_Model)
all(coef(intModel) == coef(charStor_intPred_Model))
```

The model is equivalent to the `intModel` above.

Now, let's test the model that uses int storage, but a categorical predictor:

```{r test-cat-models}
intStor_catPred_Model <- rxLinMod(ArrDelay ~ CRSDepTime + F(DayOfWeek, 1, 7), data = airlineWithIntCol)
coef(factorModel)
coef(intStor_catPred_Model)
all(coef(factorModel)[!is.na(coef(factorModel))] == coef(intStor_catPred_Model)[!is.na(coef(intStor_catPred_Model))])
```

Ok, the four different models are equivalent. Now let's benchmark:

```{r run-microbenchmark}
rxOptions(reportProgress = 0)
mb_results <- microbenchmark(
charStorage_factorModel = rxLinMod(ArrDelay ~ CRSDepTime + DayOfWeek,             data = airlineTable, reportProgress = 0),
intStorage_factorModel  = rxLinMod(ArrDelay ~ CRSDepTime + F(DayOfWeek, 1, 7),    data = airlineWithIntCol, reportProgress = 0),
charStorage_intModel    = rxLinMod(ArrDelay ~ CRSDepTime + as.integer(DayOfWeek), data = airlineTable, reportProgress = 0),
intStorage_intModel     = rxLinMod(ArrDelay ~ CRSDepTime + DayOfWeek,             data = airlineWithIntCol, reportProgress = 0),
times = bench_times
)
```

Let's now see the results and visualize it with ggplot2.

```{r load-ggplot2}
if(!require(ggplot2)){
	install.packages('ggplot2')
	library(ggplot2)
}
```

Let's plot it

```{r plot-mb-results}
mb_results
autoplot(mb_results)
save(mb_results, file = 'mb_results.Rda')
```

charStorage does make it take longer, and it's not the creation of the extra variables in the model.

So, it looks like storage method does in fact make a difference...

If we wanted to actually run more, and look at either effect sizes or statistical significance: 

```{r aov-example}
varsMat <- matrix(unlist(strsplit(as.character(mb_results$expr), split = '_')), ncol = 2, byrow = TRUE)
mb_results$storageType <- factor(varsMat[,1])
mb_results$modelType <- factor(varsMat[,2])
myAov <- aov(time/1e9 ~ storageType*modelType, data = mb_results)

summary(myAov)
```

A few more investigations of these effects and ancillary options can be found in `perftuning_follow-ups.Rmd.`

